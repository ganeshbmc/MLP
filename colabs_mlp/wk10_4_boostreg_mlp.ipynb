{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting techniques to solve regression problems  \n",
    "\n",
    "`California housing dataset`  \n",
    "[video link](https://youtu.be/yJjCDkjNNaM)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three regressors will be demonstrated:  \n",
    "* `AdaBoost regressor`  \n",
    "* `Gradient boosting regressor`  \n",
    "* `XGBoost regressor`  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import basic libraries**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(306)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use `ShuffleSplit` as cv with 10 splits and 20% examples set aside as test examples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data and split into training and test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset\n",
    "features, labels = fetch_california_housing(as_frame=True, return_X_y=True)\n",
    "labels *= 100\n",
    "\n",
    "# train-test split\n",
    "com_train_features, test_features, com_train_labels, test_labels = train_test_split(\n",
    "    features, labels, random_state=42)\n",
    "\n",
    "# train --> train + devs split\n",
    "train_features, dev_features, train_labels, dev_labels = train_test_split(\n",
    "    com_train_features, com_train_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training different regressors  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_regressor(estimator, X_train, y_train, cv, name):\n",
    "    cv_results = cross_validate(estimator,\n",
    "                                X_train,\n",
    "                                y_train,\n",
    "                                cv=cv,\n",
    "                                scoring='neg_mean_absolute_error',\n",
    "                                return_train_score=True,\n",
    "                                return_estimator=True)\n",
    "    cv_train_error = -1 * cv_results['train_score']\n",
    "    cv_test_error = -1 * cv_results['test_score']\n",
    "\n",
    "    print(f\"On an average, {name} makes an error of \"\n",
    "            f\"{cv_train_error.mean():.3f}k +/- {cv_train_error.std():.3f}k on the training set.\")\n",
    "    print(f\"On an average, {name} makes an error of \"\n",
    "            f\"{cv_test_error.mean():.3f}k +/- {cv_test_error.std():.3f}k on the test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoostRegressor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On an average, AdaBoostRegressor makes an error of 73.263k +/- 6.031k on the training set.\n",
      "On an average, AdaBoostRegressor makes an error of 73.623k +/- 6.057k on the test set.\n"
     ]
    }
   ],
   "source": [
    "#@title AdaBoostRegressor\n",
    "train_regressor(\n",
    "    AdaBoostRegressor(), com_train_features, com_train_labels,\n",
    "    cv, 'AdaBoostRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GradientBoostingRegressor  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On an average, GradientBoostingRegressor makes an error of 35.394k +/- 0.273k on the training set.\n",
      "On an average, GradientBoostingRegressor makes an error of 36.773k +/- 0.723k on the test set.\n"
     ]
    }
   ],
   "source": [
    "#@title GradientBoostingRegressor\n",
    "train_regressor(\n",
    "    GradientBoostingRegressor(), com_train_features, com_train_labels,\n",
    "    cv, 'GradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extreme gradient boosting (XGBoost) is the latest boosting technique. It is more regularized form of gradient boosting. With regularization, it is able to achieve better generalization performance than gradient boosting.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "#@title XGBoost  \n",
    "from xgboost import XGBRegressor\n",
    "xgb_regressor = XGBRegressor(objecive='reg:squarederror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mInit signature:\u001b[0m\n",
      "\u001b[0mXGBRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m*\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mobjective\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'reg:squarederror'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m     \n",
      "Implementation of the scikit-learn API for XGBoost regression.\n",
      "\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "\n",
      "    n_estimators : int\n",
      "        Number of gradient boosted trees.  Equivalent to number of boosting\n",
      "        rounds.\n",
      "\n",
      "    max_depth :  Optional[int]\n",
      "        Maximum tree depth for base learners.\n",
      "    learning_rate : Optional[float]\n",
      "        Boosting learning rate (xgb's \"eta\")\n",
      "    verbosity : Optional[int]\n",
      "        The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "    objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      "        Specify the learning task and the corresponding learning objective or\n",
      "        a custom objective function to be used (see note below).\n",
      "    booster: Optional[str]\n",
      "        Specify which booster to use: gbtree, gblinear or dart.\n",
      "    tree_method: Optional[str]\n",
      "        Specify which tree method to use.  Default to auto.  If this parameter\n",
      "        is set to default, XGBoost will choose the most conservative option\n",
      "        available.  It's recommended to study this option from the parameters\n",
      "        document: https://xgboost.readthedocs.io/en/latest/treemethod.html.\n",
      "    n_jobs : Optional[int]\n",
      "        Number of parallel threads used to run xgboost.  When used with other Scikit-Learn\n",
      "        algorithms like grid search, you may choose which algorithm to parallelize and\n",
      "        balance the threads.  Creating thread contention will significantly slow down both\n",
      "        algorithms.\n",
      "    gamma : Optional[float]\n",
      "        Minimum loss reduction required to make a further partition on a leaf\n",
      "        node of the tree.\n",
      "    min_child_weight : Optional[float]\n",
      "        Minimum sum of instance weight(hessian) needed in a child.\n",
      "    max_delta_step : Optional[float]\n",
      "        Maximum delta step we allow each tree's weight estimation to be.\n",
      "    subsample : Optional[float]\n",
      "        Subsample ratio of the training instance.\n",
      "    colsample_bytree : Optional[float]\n",
      "        Subsample ratio of columns when constructing each tree.\n",
      "    colsample_bylevel : Optional[float]\n",
      "        Subsample ratio of columns for each level.\n",
      "    colsample_bynode : Optional[float]\n",
      "        Subsample ratio of columns for each split.\n",
      "    reg_alpha : Optional[float]\n",
      "        L1 regularization term on weights (xgb's alpha).\n",
      "    reg_lambda : Optional[float]\n",
      "        L2 regularization term on weights (xgb's lambda).\n",
      "    scale_pos_weight : Optional[float]\n",
      "        Balancing of positive and negative weights.\n",
      "    base_score : Optional[float]\n",
      "        The initial prediction score of all instances, global bias.\n",
      "    random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      "        Random number seed.\n",
      "\n",
      "        .. note::\n",
      "\n",
      "           Using gblinear booster with shotgun updater is nondeterministic as\n",
      "           it uses Hogwild algorithm.\n",
      "\n",
      "    missing : float, default np.nan\n",
      "        Value in the data which needs to be present as a missing value.\n",
      "    num_parallel_tree: Optional[int]\n",
      "        Used for boosting random forest.\n",
      "    monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      "        Constraint of variable monotonicity.  See tutorial for more\n",
      "        information.\n",
      "    interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      "        Constraints for interaction representing permitted interactions.  The\n",
      "        constraints must be specified in the form of a nest list, e.g. [[0, 1],\n",
      "        [2, 3, 4]], where each inner list is a group of indices of features\n",
      "        that are allowed to interact with each other.  See tutorial for more\n",
      "        information\n",
      "    importance_type: Optional[str]\n",
      "        The feature importance type for the feature_importances\\_ property:\n",
      "\n",
      "        * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      "          \"total_cover\".\n",
      "        * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      "          without bias.\n",
      "\n",
      "    gpu_id : Optional[int]\n",
      "        Device ordinal.\n",
      "    validate_parameters : Optional[bool]\n",
      "        Give warnings for unknown parameter.\n",
      "    predictor : Optional[str]\n",
      "        Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      "        gpu_predictor].\n",
      "    enable_categorical : bool\n",
      "\n",
      "        .. versionadded:: 1.5.0\n",
      "\n",
      "        Experimental support for categorical data.  Do not set to true unless you are\n",
      "        interested in development. Only valid when `gpu_hist` and dataframe are used.\n",
      "\n",
      "    kwargs : dict, optional\n",
      "        Keyword arguments for XGBoost Booster object.  Full documentation of\n",
      "        parameters can be found here:\n",
      "        https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "        Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      "        dict simultaneously will result in a TypeError.\n",
      "\n",
      "        .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "\n",
      "            \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      "            that parameters passed via this argument will interact properly\n",
      "            with scikit-learn.\n",
      "\n",
      "        .. note::  Custom objective function\n",
      "\n",
      "            A custom objective function can be provided for the ``objective``\n",
      "            parameter. In this case, it should have the signature\n",
      "            ``objective(y_true, y_pred) -> grad, hess``:\n",
      "\n",
      "            y_true: array_like of shape [n_samples]\n",
      "                The target values\n",
      "            y_pred: array_like of shape [n_samples]\n",
      "                The predicted values\n",
      "\n",
      "            grad: array_like of shape [n_samples]\n",
      "                The value of the gradient for each sample point.\n",
      "            hess: array_like of shape [n_samples]\n",
      "                The value of the second derivative for each sample point\n",
      "\u001b[1;31mFile:\u001b[0m           c:\\programdata\\anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\sklearn.py\n",
      "\u001b[1;31mType:\u001b[0m           type\n",
      "\u001b[1;31mSubclasses:\u001b[0m     XGBRFRegressor\n"
     ]
    }
   ],
   "source": [
    "?XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:37] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:40] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:41] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:43] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:45] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:47] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:49] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:51] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:54] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\tf\\lib\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:15:59] WARNING: ..\\src\\learner.cc:576: \n",
      "Parameters: { \"objecive\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "On an average, XGBoostRegressor makes an error of 18.308k +/- 0.182k on the training set.\n",
      "On an average, XGBoostRegressor makes an error of 31.845k +/- 0.753k on the test set.\n"
     ]
    }
   ],
   "source": [
    "train_regressor(\n",
    "    xgb_regressor, com_train_features, com_train_labels,\n",
    "    cv, 'XGBoostRegressor')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "78ddfc3686b8b7161f2836984651df038ec9a0366954334fc42499f59ad2b3c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
