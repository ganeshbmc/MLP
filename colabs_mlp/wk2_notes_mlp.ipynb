{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"wk2_notes_mlp.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMFFMk7mNrMyl8EOTWVVmoZ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Data preprocessing**"],"metadata":{"id":"nIQIhJncqT08"}},{"cell_type":"markdown","source":["## 1. Feature extraction  \n","\n","> DictVectorizer  \n","> FeatureHasher  "],"metadata":{"id":"r5BppYMGqZv6"}},{"cell_type":"markdown","source":["## 2. Data cleaning  \n","\n","Handling missing values (sklearn.impute)  \n","\n","> SimpleImputer  \n","> KNNImputer  \n","\n","Note: MissingIndicator provides indicators for missing values.  \n"],"metadata":{"id":"2XY_mbEqqZqN"}},{"cell_type":"markdown","source":["## Numeric transformers  \n","\n","> Feature scaling  \n","> Polynomial transformation  \n","> Discretization  "],"metadata":{"id":"b8oktXfwqZk3"}},{"cell_type":"markdown","source":["### Feature scaling  \n","\n","> StandardScaler  \n","> MinMaxScaler   \n","> MaxAbsScaler  \n","> FunctionTransformer (use custom function for scaling. Eg. log2)  "],"metadata":{"id":"4c9SqcY5qZcg"}},{"cell_type":"markdown","source":["### Polynomial transformation  \n","\n","Generates a new feature matrix consisting of all polynomial\n","combinations of the features with degree less than or equal\n","to the speciﬁed degree.  \n","\n","> PolynomialFeatures(degree=n)  \n",">> where 'n' is the desired degree of polynomial  "],"metadata":{"id":"fJxBxaHttsri"}},{"cell_type":"markdown","source":["### Discretization  \n","\n","> KBinsDiscretizer(n_bins=n, strategy='uniform', encode='ordinal')  \n"],"metadata":{"id":"9TZzAS1ztsin"}},{"cell_type":"markdown","source":["## Categorical transformers  \n","> Feature encoding  \n","> Label encoding  \n","\n","Transformers  \n","> OneHotEncoder  \n","> LabelEncoder (can transform only one-dimensional data)  \n","> OrdinalEncoder (can operate on multidimensional data)  \n","> LabelBinarizer (can transform only one-dimensional data)  \n","> MultiLabelBinarizer  \n","\n","> add_dummy_feature"],"metadata":{"id":"UI4lYJ8ztsO4"}},{"cell_type":"markdown","source":["# **Feature selection**  \n","\n","`sklearn.feature_selection`\n","\n","> Filter based  \n","> Wrapper based  \n","\n","Note: Tree based and kernel based feature selection algorithms\n","will be covered in later weeks.  "],"metadata":{"id":"JsFDY8Wdv9wK"}},{"cell_type":"markdown","source":["### Filter based feature selection methods  \n","\n","> VarianceThreshold  \n","\n","> Univariate feature selection  \n","\n",">> SelectKbest  \n",">> SelectPercentile  \n","\n",">> SelectFpr (false postive rate)  \n",">> SelectFdr (false discovery rate)  \n",">> Select Fwe (family-wise error rate)  \n","\n",">> GenericUnivariateSelect (can implement any of the above five methods by using the kwarg 'mode')  \n","\n","Each of the above univariate feature selection methods use some common univariate statistical tests and univariate scoring functions. They do these tests/scoring by comparing a single feature with the label.  \n","\n","Univariate scoring function:  \n","Three classes of scoring functions proposed:  \n","\n","> Mutual information (MI)  \n",">> mutual_info_regression  \n",">> mutual_info_classif  \n","\n","> Chi-square  \n",">> chi2  \n","\n","> F-statistics  \n",">> f_regression  \n",">> f_classif  \n","\n","**Note**: MI and F-statistics can be used in both classification and regression problems whereas chi-square can be used only in classification problems.  \n","\n","**IMPORTANT**: Do not use regression feature scoring\n","function with a classiﬁcation problem. It will\n","lead to useless results."],"metadata":{"id":"FM0pJAPLv9hU"}},{"cell_type":"markdown","source":["### Wrapper based feature selection methods  \n","\n","**Note**: Unlike ﬁlter based methods, wrapper based\n","methods use estimator class rather than a\n","scoring function.  \n","\n","> RFE (Recursive Feature Elimination) - Need to specify the number of features desired.  \n",">> RFECV (RFE with cross-validation) - Used when we do not want to specify the number of features desired.  \n","\n","> `SelectFromModel`  \n","\n","Note: Both of the above obtain feature importance from coef_, feature_importances_ or an importance_getter callable from the trained estimator  \n","\n","> SFS (Sequential Feature Selection)  \n",">> Forward selection  \n",">> Backward selection  \n","\n","Note: Choice of forward or backward depends on original number of features and desired number of features based on efficiency. May not yield equivalent results. Does not require the underlying model to expose a coef_ or feature_importances_ attributes unlike in RFE and SelectFromModel.  "],"metadata":{"id":"KyFNVra3v9Zq"}},{"cell_type":"markdown","source":["## Applying transformations to diverse features  \n","\n","\n"],"metadata":{"id":"LEUK53toBHjt"}},{"cell_type":"markdown","source":["Composite Transformer  \n","`sklearn.compose` \n","\n","> ColumnTransformer  \n","> TransformedTargetRegressor  "],"metadata":{"id":"Tj6nYrnKBHga"}},{"cell_type":"markdown","source":["## **Dimensionality reduction**  \n"],"metadata":{"id":"KuXsBr_VBHc-"}},{"cell_type":"markdown","source":["Another way to reduce the number of feature\n","is through unsupervised dimensionality\n","reduction techniques.  \n","\n","`sklearn.decomposition`  "],"metadata":{"id":"EI1TDG3WBHWI"}},{"cell_type":"markdown","source":["### PCA (Principle component analysis)  \n","\n","`sklearn.decomposition.PCA`  \n","\n"],"metadata":{"id":"CRfIMwp4I54g"}},{"cell_type":"markdown","source":["## **Chaining transformers**  \n"],"metadata":{"id":"uVq3QLCJI5rT"}},{"cell_type":"markdown","source":["`sklearn.pipeline`  \n","\n","> Pipeline  \n",">> Pipeline()  \n",">> make_pipeline  \n","\n","> FeatureUnion  "],"metadata":{"id":"v9Wf5gkfJRs7"}}]}